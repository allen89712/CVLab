{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n",
      "batch (128, 28, 28) (128,)\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_50 (Dense)             (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 10)                170       \n",
      "=================================================================\n",
      "Total params: 244,890\n",
      "Trainable params: 244,890\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "0 0 loss: 2.3159852027893066 0.23518919944763184\n",
      "0 100 loss: 1.956762671470642 0.05045616626739502\n",
      "0 200 loss: 1.760420799255371 0.028802981600165367\n",
      "0 300 loss: 1.7619707584381104 0.03089926764369011\n",
      "0 400 loss: 1.7390588521957397 0.028616448864340782\n",
      "0 acc: 0.8239\n",
      "1 0 loss: 1.7626852989196777 0.03174250200390816\n",
      "1 100 loss: 1.6709755659103394 0.020803190767765045\n",
      "1 200 loss: 1.6735492944717407 0.019771071150898933\n",
      "1 300 loss: 1.6881346702575684 0.02532074972987175\n",
      "1 400 loss: 1.6481425762176514 0.01978713646531105\n",
      "1 acc: 0.8628\n",
      "2 0 loss: 1.658542275428772 0.025814535096287727\n",
      "2 100 loss: 1.6175098419189453 0.019606394693255424\n",
      "2 200 loss: 1.601973295211792 0.014534186571836472\n",
      "2 300 loss: 1.6409833431243896 0.019019000232219696\n",
      "2 400 loss: 1.6305807828903198 0.021381687372922897\n",
      "2 acc: 0.8659\n",
      "3 0 loss: 1.6251535415649414 0.016866235062479973\n",
      "3 100 loss: 1.6466059684753418 0.017053406685590744\n",
      "3 200 loss: 1.6280659437179565 0.01616651937365532\n",
      "3 300 loss: 1.6100906133651733 0.017148088663816452\n",
      "3 400 loss: 1.6309735774993896 0.01673886552453041\n",
      "3 acc: 0.8656\n",
      "4 0 loss: 1.6954426765441895 0.02152145653963089\n",
      "4 100 loss: 1.6215755939483643 0.016096217557787895\n",
      "4 200 loss: 1.6094691753387451 0.013876376673579216\n",
      "4 300 loss: 1.6206235885620117 0.01967916078865528\n",
      "4 400 loss: 1.6451919078826904 0.021011164411902428\n",
      "4 acc: 0.8681\n",
      "5 0 loss: 1.6343841552734375 0.016467085108160973\n",
      "5 100 loss: 1.5883876085281372 0.014775908552110195\n",
      "5 200 loss: 1.5942859649658203 0.013356138952076435\n",
      "5 300 loss: 1.627478837966919 0.015644215047359467\n",
      "5 400 loss: 1.6122194528579712 0.017262892797589302\n",
      "5 acc: 0.8805\n",
      "6 0 loss: 1.5965230464935303 0.01602892577648163\n",
      "6 100 loss: 1.620311975479126 0.018264085054397583\n",
      "6 200 loss: 1.6220831871032715 0.01625472493469715\n",
      "6 300 loss: 1.6065278053283691 0.01313536986708641\n",
      "6 400 loss: 1.6203595399856567 0.013830158859491348\n",
      "6 acc: 0.8772\n",
      "7 0 loss: 1.5598485469818115 0.007964194752275944\n",
      "7 100 loss: 1.5795806646347046 0.012677408754825592\n",
      "7 200 loss: 1.5774309635162354 0.013443260453641415\n",
      "7 300 loss: 1.5893055200576782 0.013762254267930984\n",
      "7 400 loss: 1.557930827140808 0.011115625500679016\n",
      "7 acc: 0.8791\n",
      "8 0 loss: 1.6086046695709229 0.019010111689567566\n",
      "8 100 loss: 1.6311968564987183 0.01864805817604065\n",
      "8 200 loss: 1.5497878789901733 0.009936167858541012\n",
      "8 300 loss: 1.6041111946105957 0.017414912581443787\n",
      "8 400 loss: 1.5676525831222534 0.011355144903063774\n",
      "8 acc: 0.8774\n",
      "9 0 loss: 1.572748064994812 0.008009521290659904\n",
      "9 100 loss: 1.5846424102783203 0.01680300198495388\n",
      "9 200 loss: 1.583963394165039 0.009666046127676964\n",
      "9 300 loss: 1.5830659866333008 0.013318240642547607\n",
      "9 400 loss: 1.5807523727416992 0.010906649753451347\n",
      "9 acc: 0.883\n",
      "10 0 loss: 1.6034471988677979 0.014569172635674477\n",
      "10 100 loss: 1.584863305091858 0.013147424906492233\n",
      "10 200 loss: 1.6602380275726318 0.02469574473798275\n",
      "10 300 loss: 1.5761570930480957 0.013604063540697098\n",
      "10 400 loss: 1.5844159126281738 0.012049593031406403\n",
      "10 acc: 0.886\n",
      "11 0 loss: 1.5752779245376587 0.010696993209421635\n",
      "11 100 loss: 1.6060720682144165 0.01611066609621048\n",
      "11 200 loss: 1.5493963956832886 0.009226403199136257\n",
      "11 300 loss: 1.566702127456665 0.01086964551359415\n",
      "11 400 loss: 1.54161536693573 0.007480350323021412\n",
      "11 acc: 0.881\n",
      "12 0 loss: 1.5662950277328491 0.009874975308775902\n",
      "12 100 loss: 1.5983260869979858 0.016024887561798096\n",
      "12 200 loss: 1.5895802974700928 0.014065622352063656\n",
      "12 300 loss: 1.5509817600250244 0.008053266443312168\n",
      "12 400 loss: 1.543486475944519 0.008227003738284111\n",
      "12 acc: 0.8864\n",
      "13 0 loss: 1.5569006204605103 0.010344327427446842\n",
      "13 100 loss: 1.5654380321502686 0.010822753421962261\n",
      "13 200 loss: 1.587418794631958 0.014891299419105053\n",
      "13 300 loss: 1.5801467895507812 0.014948012307286263\n",
      "13 400 loss: 1.54542076587677 0.007614067755639553\n",
      "13 acc: 0.8887\n",
      "14 0 loss: 1.568562388420105 0.011095067486166954\n",
      "14 100 loss: 1.5754523277282715 0.010791081935167313\n",
      "14 200 loss: 1.5506664514541626 0.00865599513053894\n",
      "14 300 loss: 1.5482728481292725 0.008913681842386723\n",
      "14 400 loss: 1.5472745895385742 0.008592648431658745\n",
      "14 acc: 0.8853\n",
      "15 0 loss: 1.541368842124939 0.00775260291993618\n",
      "15 100 loss: 1.5917794704437256 0.014399226754903793\n",
      "15 200 loss: 1.5546616315841675 0.012392580509185791\n",
      "15 300 loss: 1.556321382522583 0.008359922096133232\n",
      "15 400 loss: 1.5802991390228271 0.015373274683952332\n",
      "15 acc: 0.8888\n",
      "16 0 loss: 1.5846326351165771 0.013050016015768051\n",
      "16 100 loss: 1.597922682762146 0.0171649232506752\n",
      "16 200 loss: 1.5508841276168823 0.01076430082321167\n",
      "16 300 loss: 1.560760498046875 0.009916076436638832\n",
      "16 400 loss: 1.5884467363357544 0.0133348498493433\n",
      "16 acc: 0.8889\n",
      "17 0 loss: 1.5829427242279053 0.012020685710012913\n",
      "17 100 loss: 1.5273926258087158 0.009818332269787788\n",
      "17 200 loss: 1.5653777122497559 0.009904244914650917\n",
      "17 300 loss: 1.5780699253082275 0.013799184933304787\n",
      "17 400 loss: 1.585198163986206 0.015529483556747437\n",
      "17 acc: 0.8925\n",
      "18 0 loss: 1.5616981983184814 0.009689489379525185\n",
      "18 100 loss: 1.5791480541229248 0.01335626095533371\n",
      "18 200 loss: 1.568695306777954 0.01300581730902195\n",
      "18 300 loss: 1.5436146259307861 0.008633739314973354\n",
      "18 400 loss: 1.5569040775299072 0.008807247504591942\n",
      "18 acc: 0.8898\n",
      "19 0 loss: 1.5454424619674683 0.00673707714304328\n",
      "19 100 loss: 1.5220823287963867 0.008030042052268982\n",
      "19 200 loss: 1.5321433544158936 0.007785300724208355\n",
      "19 300 loss: 1.5447139739990234 0.008263261057436466\n",
      "19 400 loss: 1.567191481590271 0.00908272061496973\n",
      "19 acc: 0.8863\n",
      "20 0 loss: 1.5667084455490112 0.013734391890466213\n",
      "20 100 loss: 1.5675886869430542 0.011442470364272594\n",
      "20 200 loss: 1.5654327869415283 0.012770472094416618\n",
      "20 300 loss: 1.563685655593872 0.012775962240993977\n",
      "20 400 loss: 1.547881841659546 0.010611029341816902\n",
      "20 acc: 0.8843\n",
      "21 0 loss: 1.5615155696868896 0.01101998332887888\n",
      "21 100 loss: 1.5623221397399902 0.00972243957221508\n",
      "21 200 loss: 1.5530834197998047 0.010746688582003117\n",
      "21 300 loss: 1.550225853919983 0.009820178151130676\n",
      "21 400 loss: 1.5642461776733398 0.010139661841094494\n",
      "21 acc: 0.8895\n",
      "22 0 loss: 1.5960125923156738 0.01593201607465744\n",
      "22 100 loss: 1.5517628192901611 0.0131087526679039\n",
      "22 200 loss: 1.5530368089675903 0.011064795777201653\n",
      "22 300 loss: 1.5635159015655518 0.010934091173112392\n",
      "22 400 loss: 1.5284066200256348 0.008174757473170757\n",
      "22 acc: 0.8817\n",
      "23 0 loss: 1.5868602991104126 0.013246346265077591\n",
      "23 100 loss: 1.5583751201629639 0.011859092861413956\n",
      "23 200 loss: 1.549588918685913 0.010460736230015755\n",
      "23 300 loss: 1.5274467468261719 0.00799513142555952\n",
      "23 400 loss: 1.5792988538742065 0.014870278537273407\n",
      "23 acc: 0.8903\n",
      "24 0 loss: 1.525529384613037 0.007051152177155018\n",
      "24 100 loss: 1.5718164443969727 0.012230397202074528\n",
      "24 200 loss: 1.576859951019287 0.013388235121965408\n",
      "24 300 loss: 1.5604815483093262 0.012463653460144997\n",
      "24 400 loss: 1.5620497465133667 0.011834840290248394\n",
      "24 acc: 0.8916\n",
      "25 0 loss: 1.5842691659927368 0.013959378935396671\n",
      "25 100 loss: 1.552343726158142 0.010100292973220348\n",
      "25 200 loss: 1.5420924425125122 0.009367963299155235\n",
      "25 300 loss: 1.5423494577407837 0.00951036810874939\n",
      "25 400 loss: 1.5502220392227173 0.00908550526946783\n",
      "25 acc: 0.8904\n",
      "26 0 loss: 1.5619845390319824 0.013091830536723137\n",
      "26 100 loss: 1.5680732727050781 0.01285567693412304\n",
      "26 200 loss: 1.5417604446411133 0.008097022771835327\n",
      "26 300 loss: 1.5364508628845215 0.007788966875523329\n",
      "26 400 loss: 1.547358512878418 0.010266494005918503\n",
      "26 acc: 0.8868\n",
      "27 0 loss: 1.5537728071212769 0.01129690557718277\n",
      "27 100 loss: 1.525698184967041 0.006102032959461212\n",
      "27 200 loss: 1.525390863418579 0.005498718470335007\n",
      "27 300 loss: 1.5154732465744019 0.0046094064600765705\n",
      "27 400 loss: 1.5482170581817627 0.008293328806757927\n",
      "27 acc: 0.8946\n",
      "28 0 loss: 1.54227614402771 0.010391801595687866\n",
      "28 100 loss: 1.5798747539520264 0.01452300138771534\n",
      "28 200 loss: 1.5190160274505615 0.00525299459695816\n",
      "28 300 loss: 1.5667372941970825 0.01424424909055233\n",
      "28 400 loss: 1.5220036506652832 0.0039034015499055386\n",
      "28 acc: 0.8919\n",
      "29 0 loss: 1.5751557350158691 0.011539863422513008\n",
      "29 100 loss: 1.564636468887329 0.010837465524673462\n",
      "29 200 loss: 1.5441633462905884 0.010401548817753792\n",
      "29 300 loss: 1.5225780010223389 0.004914581775665283\n",
      "29 400 loss: 1.5380555391311646 0.008130395784974098\n",
      "29 acc: 0.8943\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, optimizers, Sequential, metrics\n",
    "\n",
    "\n",
    "def preprocess(x, y):\n",
    "    x = tf.cast(x, dtype=tf.float32) / 255.\n",
    "    y = tf.cast(y, dtype=tf.int32)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "(x, y), (x_test, y_test) = datasets.fashion_mnist.load_data()\n",
    "print(x.shape, y.shape)     # check\n",
    "\n",
    "batchsz = 128\n",
    "\n",
    "db = tf.data.Dataset.from_tensor_slices((x, y))  # 訓練\n",
    "db = db.map(preprocess).shuffle(10000).batch(batchsz)\n",
    "\n",
    "db_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))  # 測試\n",
    "db_test = db_test.map(preprocess).batch(batchsz)\n",
    "\n",
    "db_iter = iter(db)      # 迭代器\n",
    "sample = next(db_iter)  # 看他的下個個迭代\n",
    "print('batch', sample[0].shape, sample[1].shape)    # sample[0]指x sample[1]指y\n",
    "\n",
    "model = Sequential([\n",
    "    layers.Dense(256, activation=tf.nn.relu),   # [b, 784] => [b, 256]\n",
    "    layers.Dense(128, activation=tf.nn.relu),   # [b, 256] => [b, 128]\n",
    "    layers.Dense(64, activation=tf.nn.relu),    # [b, 128] => [b, 64]\n",
    "    layers.Dense(32, activation=tf.nn.relu),    # [b, 64] => [b, 32]\n",
    "    layers.Dense(10)                            # [b, 32] => [b, 10]\n",
    "])\n",
    "\n",
    "model.build(input_shape=[None, 28*28])\n",
    "model.summary()\n",
    "optimizers = optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "\n",
    "def main():\n",
    "    for epoch in range(30):\n",
    "\n",
    "        for step, (x, y) in enumerate(db):\n",
    "            # x: [b, 28, 28] => [b, 784]\n",
    "            # y: [b]\n",
    "            x = tf.reshape(x, [-1, 28*28])\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                # [b, 784] => [b, 10]\n",
    "                logits = model(x)\n",
    "                y_onehot = tf.one_hot(y, depth=10)\n",
    "                # [b]\n",
    "                loss_mse = tf.reduce_mean(tf.losses.MSE(y_onehot, logits))\n",
    "                loss_ce = tf.reduce_mean(tf.losses.categorical_crossentropy(y_onehot, logits, from_logits=True))\n",
    "\n",
    "            grads = tape.gradient(loss_mse, model.trainable_variables)\n",
    "            optimizers.apply_gradients(zip(grads, model.trainable_variables))\n",
    "            # zip(x, y)可以將x[0],y[0]拚再一起舉例  list1 = [1, 2, 3]\n",
    "            # list2 = ['a', 'b', 'c']\n",
    "            # zipped = zip(list1, list2)\n",
    "            # print(list(zipped))  # 輸出：[(1, 'a'), (2, 'b'), (3, 'c')]\n",
    "            # optimizer.apply_gradients(grads_and_vars, global_step=None,\n",
    "            # name = None)\n",
    "            # 其中grads_and_vars需要的即為一個由梯度和對應的參數組成的配對列表\n",
    "            # 組合下來即可原地根據梯度更新參數 w = w -lr*grad\n",
    "            if step % 100 == 0:\n",
    "                print(epoch, step, 'loss:', float(loss_ce), float(loss_mse))\n",
    "        # test\n",
    "        total_correct = 0\n",
    "        total_num = 0\n",
    "        for x, y in db_test:\n",
    "            # x: [b, 28, 28] => [b, 784]\n",
    "            # y: [b]\n",
    "            x = tf.reshape(x, [-1, 28*28])\n",
    "            logits = model(x)\n",
    "            #logits => prob [b,10]\n",
    "            prob = tf.nn.softmax(logits, axis=1)\n",
    "            # [b, 10] => [b]\n",
    "            pred = tf.argmax(prob, axis=1)\n",
    "            pred = tf.cast(pred, dtype=tf.int32)\n",
    "            # pred:[b]\n",
    "            # y: [b]\n",
    "            correct = tf.equal(pred, y)\n",
    "            correct = tf.reduce_sum(tf.cast(correct, dtype=tf.int32))\n",
    "            total_correct += int(correct)\n",
    "            total_num += x.shape[0]\n",
    "            \n",
    "        acc = total_correct / total_num\n",
    "        print(epoch, 'acc:', acc)\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
